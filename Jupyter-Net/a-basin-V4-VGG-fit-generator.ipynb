{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## VGG16 и VGG19 проверка результативности (реализации взяты с github keras/application):\n",
    "##                  https://github.com/fchollet/keras/tree/master/keras/applications\n",
    "##\n",
    "## 2017-05-20\n",
    "## maximum on LB => 0.84500\n",
    "##\n",
    "## TIF - (,,6) (,,3)\n",
    "##\n",
    "## 2017-07-06\n",
    "##  TIF (64,64,6) - полная схема без выемки 09(cloudy) - то есть 17 признаков\n",
    "##  \n",
    "## 2017-07-09\n",
    "##  TIF (64,64,6) - полная схема по признакам 4,5,6,7\n",
    "##\n",
    "## 2017-07-12\n",
    "##  JPG+TIF (64x64x)==(128x128x) по результатам\n",
    "##  BatchNarmalizstion не  поднимает результат\n",
    "##  Наиболее быстрый и адекватный вариант классификационной части dense1024+dense1024\n",
    "##  Ввод новых вегетационных признаков типа CCCI не приводит к улучшению. Либо также либо чуть хуже\n",
    "##\n",
    "## 2017-07-12\n",
    "##  Проверяю полный прогон на VGG19 (для очистки совести и перехожу на другое)\n",
    "##\n",
    "## 2017-07-12\n",
    "##  Проверил! LB=0.90004. Вставил VGG19 на наборе jpg128 + подставил насильственно веса от imagenet. Работает на train 9 часов.\n",
    "##  Необходима специализированная препроцессорная обработка для imagenet (RGB-->BGR; минус спец среднее из изображений)\n",
    "##\n",
    "## 2017-07-15\n",
    "##  Вернулся после неудач с другим сетями\n",
    "##\n",
    "## 2017-07-15\n",
    "##\n",
    "##  Попытка увеличить качество входного материала (resize+equalize) через fit-generator\n",
    "##\n",
    "##\n",
    "##\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys,os,datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "0.19.2\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__);\n",
    "print(pd.__version__);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  cv2 as cv\n",
    "cv.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../Python')\n",
    "from helper import paths_input, formImExt, formImHist\n",
    "from estimate import confusion_matrix, getConfusion, getRocAUC, getProb01, getProbX01, getTh, estimateResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential,save_model,load_model, Model, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Convolution1D, MaxPooling1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.optimizers\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Data/train-tif-v2',\n",
       " '../Data/test-tif-v2',\n",
       " '../Data/test-jpg-v2',\n",
       " '../Work/Train',\n",
       " '../Work/Test')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trLabels,trDirTIF,trDirJPG,teDirTIF,teDirJPG = paths_input()\n",
    "trDirI = trDirTIF\n",
    "teDirI = teDirTIF\n",
    "trWork, teWork = '../Work/Train', '../Work/Test'\n",
    "trDirI,teDirI, teDirJPG, trWork, teWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv(trLabels)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build list with unique labels\n",
    "label_list = []\n",
    "for tag_str in labels_df.tags.values:\n",
    "    labels = tag_str.split(' ')\n",
    "    for label in labels:\n",
    "        if label not in label_list:\n",
    "            label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add onehot features for every label\n",
    "for label in label_list:\n",
    "    labels_df[label] = labels_df['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n",
    "    #labels_df[label].astype(np.int8)\n",
    "# Display head\n",
    "#labels_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\n",
    "land_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation' ]\n",
    "rare_labels = [l for l in label_list if labels_df[label_list].sum()[l] < 2000]\n",
    "#rare_labels              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = label_list; #weather_labels;\n",
    "nameList =labels_df[labels_df[labels].sum(axis=1)>0].image_name.tolist(); len(nameList)\n",
    "labelList=labels_df[labels_df[labels].sum(axis=1)>0][labels].as_matrix();\n",
    "labelList[:6,:]\n",
    "#labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_df.tags.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!rm ../Temp/Batch/*.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: сделать все файлы одинаковыми, размера size. Касается последнего, связано с краевыми круговой эффектами выборки в конце.\n",
    "\n",
    "def generate_batches ( names_tags_df, dirIn, output_shape, classes=17, ext='.jpg', size=5000) :\n",
    "    xx = np.zeros((size,output_shape[0],output_shape[1],output_shape[2]),dtype=np.float32);\n",
    "    yy = np.zeros((size,classes),dtype=np.uint8);\n",
    "    \n",
    "    iijj = 0;\n",
    "    \n",
    "    for ii in range(0,len(names_tags_df),size) :\n",
    "        yy    = labels_df[ii:min(ii+size,len(names_tags_df))].values[:,2:].astype(np.uint8);\n",
    "        files = labels_df[ii:min(ii+size,len(names_tags_df))].image_name.tolist()\n",
    "        files = [x+ext for x in files]\n",
    "        if len(files)<size :\n",
    "            xx = np.zeros((len(files),output_shape[0],output_shape[1],output_shape[2]),dtype=np.float32);\n",
    "        for i in range(len(files)) :\n",
    "            nf    = os.path.join(dirIn,files[i])\n",
    "            xx[i] = formImExt(nf,EqualizeOK=False,resize=(output_shape[0],output_shape[1]))\n",
    "            iijj += 1\n",
    "            if (iijj%5000)==0 : print (datetime.datetime.now(),'-->',iijj)\n",
    "        xx    = preprocess_input(xx)\n",
    "        filexx= '../Temp/BatchN/batch-{}-{}x{}x{}-XX.npy'.format(ii,output_shape[0],output_shape[1],output_shape[2])\n",
    "        fileyy= '../Temp/BatchN/batch-{}-{}x{}x{}-YY.npy'.format(ii,output_shape[0],output_shape[1],output_shape[2])\n",
    "        np.save(filexx,xx)\n",
    "        np.save(fileyy,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-16 13:47:56.670557\n",
      "2017-07-16 13:50:17.542210 --> 5000\n",
      "2017-07-16 13:52:10.737408 --> 10000\n",
      "2017-07-16 13:54:39.562434 --> 15000\n",
      "2017-07-16 13:57:12.832186 --> 20000\n",
      "2017-07-16 13:59:24.678649 --> 25000\n",
      "2017-07-16 14:01:55.349893 --> 30000\n",
      "2017-07-16 14:04:04.832081 --> 35000\n",
      "2017-07-16 14:06:30.966921 --> 40000\n",
      "2017-07-16 14:07:18.148274\n"
     ]
    }
   ],
   "source": [
    "# Препростройка для быстрой подачи batches\n",
    "print(datetime.datetime.now())\n",
    "##generate_batches(labels_df,trDirJPG,(224,224,3))\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_train (batch_size, n0, ns, dirIn='../Temp/BatchN', size=5000, output_shape=(224,224,3), max_fi=40000, debugOK=False):\n",
    "    \n",
    "    def get_new_fi (fi, bi, gi) :\n",
    "        fi_old = fi\n",
    "        while ((bi+batch_size)>size) or ((gi+1)>=n0+ns) : \n",
    "            fi  = (fi + size) if ((gi<(n0+ns))and ((fi+size)<=max_fi)) else (n0/size)*size;\n",
    "            bi  = (n0-fi) if fi<=n0 else 0\n",
    "            gi  = fi+bi\n",
    "        return (fi,bi,gi)\n",
    "        \n",
    "    \n",
    "    assert (batch_size<=size)\n",
    "    assert ((max_fi/size)>1) or ((n0+batch_size)<=size)\n",
    "        \n",
    "    while 1:\n",
    "        \n",
    "        fi       = (n0/size)*size\n",
    "        bi       = (n0-fi) if fi<=n0 else 0\n",
    "        gi       = fi+bi\n",
    "        \n",
    "        fi,bi,gi = get_new_fi (fi,bi,gi)\n",
    "        \n",
    "        if debugOK : print(datetime.datetime.now(),'g read fi={} bi={} gi={} batch_size={} n0={} ns={} dirIn={}'.\n",
    "                           format(fi,bi,gi,batch_size,n0,ns,dirIn))\n",
    "            \n",
    "        fx = 'batch-{}-{}x{}x{}-XX.npy'\n",
    "        fy = 'batch-{}-{}x{}x{}-YY.npy'\n",
    "        xx = np.load(os.path.join(dirIn,fx.format(fi,output_shape[0],output_shape[1],output_shape[2])));\n",
    "        yy = np.load(os.path.join(dirIn,fy.format(fi,output_shape[0],output_shape[1],output_shape[2])));\n",
    "        old_fi = fi\n",
    "    \n",
    "        while True :\n",
    "            if ((bi+batch_size)>yy.shape[0]) or ((gi+1)>=n0+ns) :\n",
    "                fi,bi,gi = get_new_fi (fi,bi,gi)\n",
    "    \n",
    "                if debugOK : print(datetime.datetime.now(),'get_train read fi={} bi={} gi={}'.format(fi,bi,gi))\n",
    "            \n",
    "                if (fi<>old_fi) :\n",
    "                    xx = np.load(os.path.join(dirIn,fx.format(fi,output_shape[0],output_shape[1],output_shape[2])));\n",
    "                    yy = np.load(os.path.join(dirIn,fy.format(fi,output_shape[0],output_shape[1],output_shape[2])));\n",
    "                    old_fi = fi\n",
    "\n",
    "                if debugOK : print(datetime.datetime.now(),'get_train xx={} yy={} fx={}'.\n",
    "                                   format(xx.shape,yy.shape,\n",
    "                                          os.path.join(dirIn,fx.format(fi,output_shape[0],output_shape[1],output_shape[2]))))\n",
    "                \n",
    "            \n",
    "            xxx, yyy, bi, gi = xx[bi:bi+batch_size], yy[bi:bi+batch_size], bi+batch_size, gi+batch_size\n",
    "            #if debugOK : print(datetime.datetime.now(),\n",
    "            #                   'get_train get  fi={} bi={} gi={} xxx={} yyy={}'.format(fi,bi,gi,xxx.shape,yyy.shape))\n",
    "            \n",
    "            yield (xxx, yyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-17 02:10:19.271595 g read fi=5000 bi=0 gi=5000 batch_size=128 n0=4873 ns=2000 dirIn=../Temp/BatchN\n",
      "2017-07-17 02:10:19.795957 get_train get  fi=5000 bi=128 gi=5128 xxx=(128, 224, 224, 3) yyy=(128, 17)\n",
      "2017-07-17 02:10:19.796059 get_train get  fi=5000 bi=256 gi=5256 xxx=(128, 224, 224, 3) yyy=(128, 17)\n",
      "2017-07-17 02:10:19.796107 get_train get  fi=5000 bi=384 gi=5384 xxx=(128, 224, 224, 3) yyy=(128, 17)\n",
      "2017-07-17 02:10:19.796158 get_train get  fi=5000 bi=512 gi=5512 xxx=(128, 224, 224, 3) yyy=(128, 17)\n"
     ]
    }
   ],
   "source": [
    "#xx = get_train(128,4873,2000,debugOK=True)\n",
    "#for ii in range(4) : xx.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "    print(datetime.datetime.now())\n",
    "    output_shape=(224,224,3)\n",
    "    for fi in range(0,40497,5000) :\n",
    "            fx = '../Temp/Batch/batch-{}-{}x{}x{}-XX.npy'\n",
    "            fy = '../Temp/Batch/batch-{}-{}x{}x{}-YY.npy'\n",
    "            xx = np.load(fx.format(fi,output_shape[0],output_shape[1],output_shape[2]));\n",
    "            yy = np.load(fy.format(fi,output_shape[0],output_shape[1],output_shape[2]));\n",
    "            print (fi,xx.shape,yy.shape)\n",
    "    print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "    for i in range(3) :\n",
    "        x000 = x00[:,i]\n",
    "        print(x000.mean(), x000.std(), len(x000[x000<0.1]),x000.argmin())\n",
    "        #plt.hist(x000[x000<0.0001].ravel(),bins=150); \n",
    "        plt.hist(x000.ravel(),bins=150); \n",
    "        plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fbeta_pred(y_true, y_pred, beta=2.0, OK1=0.2, eps=0.000001, printOK=False):\n",
    "    beta2 = beta*beta\n",
    "    yy_true = K.round(y_true)\n",
    "    #yy_pred = K.round(y_pred+(0.5-OK1))\n",
    "    yy_pred = K.round(y_pred)\n",
    "    tp, tp_fp, fn = K.sum((yy_pred*yy_true)), K.sum(yy_true), K.sum((K.abs(yy_pred*(yy_true-1.0))))\n",
    "    precision, recall = tp/(tp_fp+eps), tp/(tp+fn+eps) \n",
    "    fbeta = (1+beta2)*(precision*recall)/(beta2*precision+recall+eps)\n",
    "    ##if fbeta>1.0 : fbeta = 1.0;\n",
    "    if printOK :\n",
    "        print('ten true ',K.get_value(yy_true))\n",
    "        #print('ten pred ',y_pred)\n",
    "        print('ten roun ',K.get_value(yy_pred))\n",
    "        print(' pre=',K.get_value(precision),' recall=',K.get_value(recall),' tp=',\n",
    "              K.get_value(tp),' fn=',K.get_value(fn),' tp+fp=',K.get_value(tp_fp))\n",
    "    return(fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3) 17\n"
     ]
    }
   ],
   "source": [
    "sizing = (224,224,3)\n",
    "input_shape, output_classes, metric = sizing, 17, 'acc'\n",
    "print(input_shape,output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model00 = VGG19(weights='imagenet', include_top=True)\n",
    "for layer in model00.layers :\n",
    "    if layer.name=='fc1' : fc1 = layer.get_weights()\n",
    "    if layer.name=='fc2' : fc2 = layer.get_weights()\n",
    "#model00.summary()\n",
    "del model00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(25088, 4096), (4096,)], [(4096, 4096), (4096,)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in fc1], [x.shape for x in fc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fc1 [(25088, 4096), (4096,)]\n",
      " fc1 fc1-0 fc1-1 fc2 fc2-0 fc2-1\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "base_model = VGG19(weights='imagenet', include_top=False, pooling=None, input_shape=input_shape)\n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "##print(base_model.summary())\n",
    "\n",
    "##x = GlobalMaxPooling2D()(x) \n",
    "x = Flatten()(x)\n",
    "initiaze = 'he_normal'\n",
    "x = Dense(4096, activation='relu',kernel_initializer=initiaze, name='ffcc1')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(4096, activation='relu',kernel_initializer=initiaze, name='ffcc2')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(output_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model10 = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "for layer in base_model.layers : layer.Tranable = False\n",
    "\n",
    "flag = ''\n",
    "for layer in model10.layers :\n",
    "    if layer.name=='ffcc1' : \n",
    "        ffcc = layer.get_weights(); flag+=' fc1'; print(flag,[x.shape for x in ffcc])\n",
    "        if (ffcc[0].shape==fc1[0].shape) : ffcc[0]+=fc1[0]; flag+=' fc1-0'\n",
    "        if (ffcc[1].shape==fc1[1].shape) : ffcc[1]+=fc1[1]; flag+=' fc1-1'\n",
    "        layer.set_weights(ffcc)\n",
    "    if layer.name=='ffcc2' :\n",
    "        ffcc = layer.get_weights(); flag+=' fc2'\n",
    "        if (ffcc[0].shape==fc2[0].shape) : ffcc[0]+=fc2[0]; flag+=' fc2-0'\n",
    "        if (ffcc[1].shape==fc2[1].shape) : ffcc[1]+=fc2[1]; flag+=' fc2-1'\n",
    "        layer.set_weights(ffcc)\n",
    "        \n",
    "print(flag)\n",
    "\n",
    "sgd = keras.optimizers.SGD(nesterov=True)\n",
    "model10.compile(loss='binary_crossentropy', # 'mean_absolute_error'\n",
    "              optimizer=sgd, #\"nadam\", #sgd, #\"adam\", #'rmsprop',\n",
    "              metrics=['acc']) #['binary_accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "\n",
    "#model10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True :\n",
    "    model10 = load_model('../Data-Keras/Models/model-VGG19-224x224x3-d4096xd4096.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False : \n",
    "    sweight  = np.zeros(trY.shape[0],dtype=np.float32);\n",
    "    sweight[:]  = 1.0\n",
    "    sweight[trY[:,0]==1] = 0.35\n",
    "    sweight[trY[:,1]==1] = 0.25\n",
    "    sweight[trY[:,2]==1] = 0.15\n",
    "    sweight[trY[:,3]==1] = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-17 02:21:47.618309\n",
      "Epoch 1/1\n",
      "2017-07-17 02:21:47.618922 g read fi=0 bi=0 gi=0 batch_size=16 n0=0 ns=4398 dirIn=../Temp/BatchN\n",
      "2017-07-17 02:26:47.062390 get_train read fi=0 bi=0 gi=0\n",
      "2017-07-17 02:26:47.062839 get_train xx=(5000, 224, 224, 3) yy=(5000, 17) fx=../Temp/BatchN/batch-0-224x224x3-XX.npy\n",
      "2017-07-17 02:26:57.117338 g read fi=0 bi=4398 gi=4398 batch_size=16 n0=4398 ns=1099 dirIn=../Temp/BatchN\n",
      "2017-07-17 02:27:29.688772 get_train read fi=5000 bi=0 gi=5000\n",
      "2017-07-17 02:29:25.364498 get_train xx=(5000, 224, 224, 3) yy=(5000, 17) fx=../Temp/BatchN/batch-5000-224x224x3-XX.npy\n",
      "2017-07-17 02:29:33.390740 get_train read fi=0 bi=4398 gi=4398\n",
      "2017-07-17 02:29:58.695041 Epoch 00000: acc improved from -inf to 0.85815, saving model to ../Temp/Temp/vgg19G-jpg-tif-224x224x3-weights.00-acc=0.8581-val_acc=0.8476-.hdf5get_train xx=(5000, 224, 224, 3) yy=(5000, 17) fx=../Temp/BatchN/batch-0-224x224x3-XX.npy\n",
      "\n",
      "500s - loss: 0.2518 - acc: 0.8581 - val_loss: 0.2514 - val_acc: 0.8476\n",
      "2017-07-17 02:30:08.840116\n"
     ]
    }
   ],
   "source": [
    "epochs     = 5\n",
    "verbose    = 2\n",
    "batch_size = 16\n",
    "stopping   = 4\n",
    "\n",
    "prefixTemp = 'vgg19G-jpg-tif-{}x{}x{}'.format(sizing[0],sizing[1],sizing[2])     \n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=stopping,min_delta=0.0001)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)\n",
    "\n",
    "filepath=\"../Temp/Temp/\"+prefixTemp+\"-weights.{epoch:02d}-acc={\"+metric+\":.4f}-val_acc={val_\"+metric+\":.4f}-.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=metric, verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "step = 5497\n",
    "low  = 0\n",
    "high = low+step\n",
    "\n",
    "xxyy = 0\n",
    "#trXX = trX #[trY[:,xxyy]==1]\n",
    "#trYY = trY #[trY[:,xxyy]==1]\n",
    "\n",
    "lb_df = labels_df[:40000] # пока последний файл не добьется поворотами изображений\n",
    "lb_df = labels_df[low:high]\n",
    "split = len(lb_df)-int(len(lb_df)/5.0)\n",
    "generate, validate = lb_df[:split], lb_df[split:]\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "#hist1  = model10.fit(trX[low:high],trY[low:high],\n",
    "#hist1  = model10.fit(trX[low:high],trY[low:high],\n",
    "hist1  = model10.fit_generator( get_train(batch_size,0,len(generate),debugOK=True),\n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=int(len(generate)/batch_size),\n",
    "                    validation_data=get_train(batch_size,split,len(validate),debugOK=True),\n",
    "                    validation_steps=int(len(validate)/batch_size),\n",
    "                    #### ???? max_queue_size=100,\n",
    "                    callbacks=[early_stopping,checkpoint, reduce_lr],\n",
    "                    verbose=verbose)\n",
    "\n",
    "##trP = model1.predict(trX, batch_size=128)\n",
    "##fbeta2score=fbeta_score(trY, np.array(trP) > 0.2, beta=2, average='samples')\n",
    "##fbeta2pred =K.get_value(fbeta_pred(trY.astype(np.float64),trP.astype(np.float64)))\n",
    "print(datetime.datetime.now()) #,pp,'fbeta2s=',fbeta2score,fbeta2pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#fc1[1].shape, fc1[1][:10], fc2[0][:10], fc2[1][:10], ffcc[0][:10], ffcc[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sgd.get_config(); hist1.history\n",
    "plt.plot(hist1.history['acc']); plt.plot(hist1.history['val_acc']); plt.show()\n",
    "plt.plot(hist1.history['loss']); plt.plot(hist1.history['val_loss']); plt.show()\n",
    "#hist1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trX.shape, trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time trP = model10.predict(trX, batch_size=batch_size)\n",
    "trP01       = getProb01(trP)\n",
    "th, _       = getTh(trY,trP)\n",
    "trP01x      = getProbX01(trP,th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 17)\n",
      "(40479, 17)\n",
      "fbeta_score= 0.235712955756\n",
      "fbeta_pred = 0.43852996984\n",
      "fbeta_score= 0.936343915303\n",
      "fbeta_pred = 0.93375767889\n"
     ]
    }
   ],
   "source": [
    "#print(model10.evaluate(trX,trY,verbose=2,batch_size=batch_size))\n",
    "print(trY.shape)\n",
    "print(trP.shape)\n",
    "#print('fbeta_score=',fbeta_score(trY, np.array(trP) > 0.2, beta=2, average='samples'))\n",
    "#print('fbeta_score=',fbeta_score(trY, np.array(trP) > 0.5, beta=2, average='samples'))\n",
    "##-------------\n",
    "print('fbeta_score=',fbeta_score(trY,trP01, beta=2, average='samples'))\n",
    "print('fbeta_pred =',K.get_value(fbeta_pred(trY.astype(np.float64),trP01.astype(np.float64))))\n",
    "##-------------\n",
    "if trY.shape[1]==17 :\n",
    "    print('fbeta_score=',fbeta_score(trY,trP01x, beta=2, average='samples'))\n",
    "    print('fbeta_pred =',K.get_value(fbeta_pred(trY.astype(np.float64),trP01x.astype(np.float64))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "    freeze  = ('fc1','fc2','dropout_1','dropout_2','flatten')\n",
    "    trainOK = ('input_1','predictions')\n",
    "    for layer in model10.layers :\n",
    "        #if (layer.name=='fc1') : layer.trainable = False\n",
    "        #if (layer.name=='fc2') : layer.trainable = False\n",
    "        if (layer.name in freeze) : layer.trainable = False\n",
    "        else : layer.trainable = True\n",
    "        if (layer.name in trainOK) : layer.trainable = True\n",
    "        layer.trainable = True\n",
    "        print(layer.name,layer.trainable)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if True :\n",
    "    save_model(model10,'../Data-Keras/Models/model-VGG19-224x224x3-d4096xd4096.h5')\n",
    "    model10.save_weights('../Data-Keras/Models/model-VGG19-224x224x3-d4096xd4096-weights.h5')\n",
    "    if False : # best 0x90004\n",
    "        save_model(model10,'../Work/Join-XGB-NET/VGG19-LB=0x90004/model-VGG19-128x128x3-d4096xd4096.h5')\n",
    "        model10.save_weights('../Work/Join-XGB-NET/VGG19-LB=0x90004/model-VGG19-128x128x3-d4096xd4096-weights.h5')\n",
    "        np.save('../Work/Join-XGB-NET/VGG19-LB=0x90004/trIP01-XX.npy',trP01)\n",
    "        np.save('../Work/Join-XGB-NET/VGG19-LB=0x90004/trIP-XX.npy', trP)\n",
    "        np.save('../Work/Join-XGB-NET/VGG19-LB=0x90004/trIY-YY.npy',trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "ffcc1 (Dense)                (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "ffcc2 (Dense)                (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17)                69649     \n",
      "=================================================================\n",
      "Total params: 70,433,873\n",
      "Trainable params: 70,433,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.70237350e-01,   4.93190736e-01,   1.06996112e-03,\n",
       "          1.33314744e-01,   6.74560608e-04,   9.84107974e-05,\n",
       "          2.12351413e-04,   4.83186363e-04,   4.88138940e-05,\n",
       "          1.42483710e-04,   7.10203385e-05,   1.37875631e-05,\n",
       "          4.31219232e-05,   1.01183177e-05,   1.92909007e-04,\n",
       "          5.11052203e-05,   1.45314596e-04], dtype=float32),\n",
       " array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.], dtype=float32),\n",
       " array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8))"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trP[0], trY[0], trP01[0], trP01x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Результативность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 acc=0.976679265792 roc=0.883524232955 not=944 yes=2093 no=37442 true=39535 all-1-0=(0.7760-0.9910)\n",
      "1 acc=0.977642728328 roc=0.883610525919 not=905 yes=37280 no=2294 true=39574 all-1-0=(0.9938-0.7734)\n",
      "2 acc=0.928926109835 roc=0.922535026704 not=2877 yes=11160 no=26442 true=37602 all-1-0=(0.9062-0.9389)\n",
      "3 acc=0.973467724005 roc=0.967815856268 not=1074 yes=27913 no=11492 true=39405 all-1-0=(0.9818-0.9539)\n",
      "4 acc=0.949825835618 roc=0.90710241146 not=2031 yes=6223 no=32225 true=38448 all-1-0=(0.8397-0.9745)\n",
      "5 acc=0.961436794387 roc=0.849248928854 not=1561 yes=2607 no=36311 true=38918 all-1-0=(0.7123-0.9862)\n",
      "6 acc=0.951258677339 roc=0.919828071166 not=1973 yes=7002 no=31504 true=38506 all-1-0=(0.8676-0.9721)\n",
      "7 acc=0.932434101633 roc=0.767210724538 not=2735 yes=2485 no=35259 true=37744 all-1-0=(0.5551-0.9794)\n",
      "8 acc=0.994960349811 roc=0.531001148943 not=204 yes=13 no=40262 true=40275 all-1-0=(0.0622-0.9998)\n",
      "9 acc=0.99137824551 roc=0.963542756305 not=349 yes=1948 no=38182 true=40130 all-1-0=(0.9325-0.9946)\n",
      "10 acc=0.988240816226 roc=0.984333394426 not=476 yes=7103 no=32900 true=40003 all-1-0=(0.9782-0.9904)\n",
      "11 acc=0.998369524939 roc=0.719876173258 not=66 yes=44 no=40369 true=40413 all-1-0=(0.4400-0.9998)\n",
      "12 acc=0.983201166037 roc=0.678198917044 not=680 yes=310 no=39489 true=39799 all-1-0=(0.3596-0.9968)\n",
      "13 acc=0.998023666593 roc=0.924417525387 not=80 yes=288 no=40111 true=40399 all-1-0=(0.8496-0.9993)\n",
      "14 acc=0.992391116381 roc=0.57199025509 not=308 yes=48 no=40123 true=40171 all-1-0=(0.1446-0.9994)\n",
      "15 acc=0.993502803923 roc=0.696348790893 not=263 yes=134 no=40082 true=40216 all-1-0=(0.3941-0.9986)\n",
      "16 acc=0.997702512414 roc=0.530599862837 not=93 yes=6 no=40380 true=40386 all-1-0=(0.0612-1.0000)\n"
     ]
    }
   ],
   "source": [
    "temp = estimateResult(trY,trP01x,printOK=True) # 40000 jpg128 (epoch=40,features=17) VGG19 d4096+d4096 +weights+imnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 acc=0.970824378073 roc=0.827535509167 not=1181 yes=1786 no=37512 true=39298 all-1-0=(0.6622-0.9929)\n",
      "1 acc=0.976555744954 roc=0.877590365212 not=949 yes=37271 no=2259 true=39530 all-1-0=(0.9935-0.7616)\n",
      "2 acc=0.917611601077 roc=0.909720294098 not=3335 yes=10955 no=26189 true=37144 all-1-0=(0.8896-0.9299)\n",
      "3 acc=0.964376590331 roc=0.949960875432 not=1442 yes=28021 no=11016 true=39037 all-1-0=(0.9856-0.9143)\n",
      "4 acc=0.937943131006 roc=0.87391779368 not=2512 yes=5728 no=32239 true=37967 all-1-0=(0.7729-0.9749)\n",
      "5 acc=0.954074952444 roc=0.807677321803 not=1859 yes=2302 no=36318 true=38620 all-1-0=(0.6290-0.9864)\n",
      "6 acc=0.942711035352 roc=0.901277662688 not=2319 yes=6718 no=31442 true=38160 all-1-0=(0.8324-0.9702)\n",
      "7 acc=0.928012055634 roc=0.768147534975 not=2914 yes=2520 no=35045 true=37565 all-1-0=(0.5629-0.9734)\n",
      "8 acc=0.994861533141 roc=0.507152201111 not=208 yes=3 no=40268 true=40271 all-1-0=(0.0144-1.0000)\n",
      "9 acc=0.990464191309 roc=0.950839323708 not=386 yes=1894 no=38199 true=40093 all-1-0=(0.9067-0.9950)\n",
      "10 acc=0.982410632674 roc=0.970772637269 not=712 yes=6917 no=32850 true=39767 all-1-0=(0.9526-0.9889)\n",
      "11 acc=0.998443637442 roc=0.759814259888 not=63 yes=52 no=40364 true=40416 all-1-0=(0.5200-0.9996)\n",
      "12 acc=0.981447170138 roc=0.627369387875 not=751 yes=222 no=39506 true=39728 all-1-0=(0.2575-0.9972)\n",
      "13 acc=0.99743076657 roc=0.910956343065 not=104 yes=279 no=40096 true=40375 all-1-0=(0.8230-0.9989)\n",
      "14 acc=0.991946441365 roc=0.543388251489 not=326 yes=29 no=40124 true=40153 all-1-0=(0.0873-0.9994)\n",
      "15 acc=0.992909903901 roc=0.647931489544 not=287 yes=101 no=40091 true=40192 all-1-0=(0.2971-0.9988)\n",
      "16 acc=0.997702512414 roc=0.545868839103 not=93 yes=9 no=40377 true=40386 all-1-0=(0.0918-0.9999)\n"
     ]
    }
   ],
   "source": [
    "temp = estimateResult(trY,trP01x,printOK=True) # 40000 jpg128 (epoch=40,features=17) VGG19 d4096+d4096 +weights+imnet LB=0.90004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('../Data-Keras/train-model-2D-2-v2-loop-weights.h5') ## verify load weights from v1 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Forming output dataset for predicting --> trOX, trOY\n",
    "del(trX)\n",
    "del(trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trOX  = np.load('../Data-Keras/Datas/test-model-2D-128x128x3-jpg-XX.npy').astype(np.float32)\n",
    "trOY  = np.load('../Data-Keras/Datas/test-model-2D-128x128x3-jpg-YY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Препроцессорная обработка данных\n",
    "\n",
    "trOX  = preprocess_input(trOX)\n",
    "\n",
    "#trOX  = trOX[:, :, :, ::-1] # RGB --> BGR\n",
    "\n",
    "##trX  = np.array(trX,dtype=np.float16)\n",
    "#trOX[:, :, :, 0] -= 103.939\n",
    "#trOX[:, :, :, 1] -= 116.779\n",
    "#trOX[:, :, :, 2] -= 123.68\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  Построение решения для test массива на основе уровней прохождения (эффект почему-то дают)\n",
    "#    Построенное решение 01 полное с учетом погодных критериев [0,3,9,10]\n",
    "#        и особенности критерия cloudy (посмотреть и сменить можно в getProbX01)\n",
    "#        Построение критериев прохождения в getTh\n",
    "#\n",
    "\n",
    "# test\n",
    "trOP = model10.predict(trOX, batch_size=batch_size)\n",
    "\n",
    "# Уровни прохождения & решение\n",
    "trO01   = getProbX01(trOP,th)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#trO01[0:3],np.array(th),trOP[0:3]\n",
    "if False :\n",
    "    np.save('../Work/Join-XGB-NET/VGG19-LB=0x90004/trO01-OX.npy',trO01)\n",
    "    np.save('../Work/Join-XGB-NET/VGG19-LB=0x90004/trOP-OX.npy', trOP)\n",
    "    np.save('../Work/Join-XGB-NET/VGG19-LB=0x90004/trOY-OY.npy',trOY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Строим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trZ = trO01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for i in range(trZ.shape[0]) :\n",
    "    temp = [labels[ii] for ii in range(trZ.shape[1]) if trZ[i,ii]==1];\n",
    "    temp = ' '.join(temp)\n",
    "    temp_list.append([trOY[i],temp])\n",
    "temp_list.sort(cmp=lambda x,y: cmp(int(x[0].partition('_')[2]),int(y[0].partition('_')[2])) if (x[0].partition('_')[0]==y[0].partition('_')[0]) else cmp(y[0].partition('_')[0],x[0].partition('_')[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['test_0', 'primary clear'],\n",
       " ['test_1', 'primary clear'],\n",
       " ['test_2', 'primary partly_cloudy'],\n",
       " ['test_3', 'primary agriculture clear'],\n",
       " ['test_4', 'cloudy']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-15-21-01-35\n"
     ]
    }
   ],
   "source": [
    "rrr=pd.DataFrame(temp_list,columns=['image_name','tags']); rrr.head(); \n",
    "suffixDT = (datetime.datetime.now()).strftime('%Y-%m-%d-%H-%M-%S'); print(suffixDT)\n",
    "rrr.to_csv('../Result/vss'+suffixDT+'.csv',index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>primary clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>primary partly_cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>primary agriculture clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                       tags\n",
       "0     test_0              primary clear\n",
       "1     test_1              primary clear\n",
       "2     test_2      primary partly_cloudy\n",
       "3     test_3  primary agriculture clear\n",
       "4     test_4                     cloudy"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Склад барахла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Skip to content\n",
    "Features Business Explore Marketplace Pricing\n",
    "This repository\n",
    "Search\n",
    "Sign in or Sign up\n",
    " Watch 1,176  Star 17,415  Fork 6,223 fchollet/keras\n",
    " Code  Issues 1,167  Pull requests 36  Projects 1  Wiki Insights \n",
    "Branch: master Find file Copy pathkeras/keras/applications/vgg19.py\n",
    "bac1637  on 22 May\n",
    "@taehoonlee taehoonlee Fix typos (#6702)\n",
    "4 contributors @fchollet @taehoonlee @singlas @ozancaglayan\n",
    "RawBlameHistory     \n",
    "193 lines (167 sloc)  8.29 KB\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"VGG19 model for Keras.\n",
    "# Reference\n",
    "- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "\n",
    "from ..models import Model\n",
    "from ..layers import Flatten\n",
    "from ..layers import Dense\n",
    "from ..layers import Input\n",
    "from ..layers import Conv2D\n",
    "from ..layers import MaxPooling2D\n",
    "from ..layers import GlobalAveragePooling2D\n",
    "from ..layers import GlobalMaxPooling2D\n",
    "from ..engine.topology import get_source_inputs\n",
    "from ..utils import layer_utils\n",
    "from ..utils.data_utils import get_file\n",
    "from .. import backend as K\n",
    "from .imagenet_utils import decode_predictions\n",
    "from .imagenet_utils import preprocess_input\n",
    "from .imagenet_utils import _obtain_input_shape\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "def VGG19(include_top=True, weights='imagenet',\n",
    "          input_tensor=None, input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=1000):\n",
    "    \"\"\"Instantiates the VGG19 architecture.\n",
    "    Optionally loads weights pre-trained\n",
    "    on ImageNet. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_data_format=\"channels_last\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The data format\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"imagenet\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 48.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      include_top=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='vgg19')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models')\n",
    "        else:\n",
    "            weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models')\n",
    "        model.load_weights(weights_path)\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name='block5_pool')\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name='fc1')\n",
    "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
    "\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "    return model\n",
    "Contact GitHub API Training Shop Blog About\n",
    "© 2017 GitHub, Inc. Terms Privacy Security Status Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def VGG16 ( include_top=True, weights='imagenet',\n",
    "          input_tensor=None, input_shape=None,\n",
    "          pooling=None,\n",
    "          classes=1000):\n",
    "    \"\"\"Instantiates the VGG16 architecture.\n",
    "    Optionally loads weights pre-trained\n",
    "    on ImageNet. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_data_format=\"channels_last\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The data format\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization)\n",
    "            or \"imagenet\" (pre-training on ImageNet).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 224)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 48.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      include_top=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor    \n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='vgg16')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models')\n",
    "        else:\n",
    "            weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models')\n",
    "        model.load_weights(weights_path)\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name='block5_pool')\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name='fc1')\n",
    "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
    "\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "    from keras.utils.data_utils import get_file\n",
    "    WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "\n",
    "    weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                        WEIGHTS_PATH,\n",
    "                                        cache_subdir='models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                        WEIGHTS_PATH,\n",
    "                        cache_subdir='models')\n",
    "weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        WEIGHTS_PATH_NO_TOP,\n",
    "                        cache_subdir='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildModel (iSize,rSize,params=None,cv2d=None,dense=None) :\n",
    "    model = Sequential()\n",
    "    if (cv2d is None) and (dense is None) and not (params is None) : cv2d, dense = params[:-2], params[-2:]\n",
    "    model = Kriz2012x3x3(model,iSize,rSize,cv2d=cv2d,dense=dense,pp=params)\n",
    "    sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.99, nesterov=True)\n",
    "    model.compile(loss='mean_absolute_error',  #'binary_crossentropy',\n",
    "                  optimizer=\"adam\", #sgd, #\"adam\", #'rmsprop',\n",
    "                  metrics=[fbeta_pred,'acc']) #['binary_accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    #sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(loss='categorical_crossentropy',\n",
    "    #              optimizer='adam', #'adam', #sgd, #\"adam\", #'rmsprop',\n",
    "    #              metrics=['accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    return(model)\n",
    "\n",
    "def buildModelKriz (iSize,rSize) :\n",
    "    model = Sequential()\n",
    "    \n",
    "    model = Kriz2012(model,iSize,rSize)\n",
    "    #sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.99, nesterov=True)\n",
    "    sgd = keras.optimizers.SGD(nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', # 'mean_absolute_error'\n",
    "                  optimizer=\"sgd\", #sgd, #\"adam\", #'rmsprop',\n",
    "                  metrics=['acc']) #['binary_accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    #sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(loss='categorical_crossentropy',\n",
    "    #              optimizer='adam', #'adam', #sgd, #\"adam\", #'rmsprop',\n",
    "    #              metrics=['accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    return(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "#from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "#model = VGG16(weights='imagenet', include_top=False)\n",
    "model10 = VGG16(weights=None,input_shape=(224,224,3),include_top=True,classes=4)\n",
    "\n",
    "#x = (Dense(512,activation='relu'))(model10)\n",
    "#model10.add(Dropout(0.25))\n",
    "#model10.add(Dense(4,activation='sigmoid'))\n",
    "\n",
    "#img_path = 'elephant.jpg'\n",
    "#img = image.load_img(img_path, target_size=(224, 224))\n",
    "#x = image.img_to_array(img)\n",
    "#x = np.expand_dims(x, axis=0)\n",
    "#x = preprocess_input(x)\n",
    "\n",
    "#features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

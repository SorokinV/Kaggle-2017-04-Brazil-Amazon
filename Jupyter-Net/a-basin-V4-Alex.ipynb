{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "## AlexNet analog (Krizhevsky 2012)\n",
    "##\n",
    "## 2017-05-20\n",
    "## maximum on LB => 0.84500\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys,os,datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "0.19.2\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__);\n",
    "print(pd.__version__);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  cv2 as cv\n",
    "cv.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../Python')\n",
    "from helper import formFH, paths_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential,save_model,load_model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Convolution1D, MaxPooling1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.optimizers\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Data/train-tif-v2',\n",
       " '../Data/test-tif-v2',\n",
       " '../Data/test-jpg-v2',\n",
       " '../Work/Train',\n",
       " '../Work/Test')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trLabels,trDirTIF,trDirJPG,teDirTIF,teDirJPG = paths_input()\n",
    "trDirI = trDirTIF\n",
    "teDirI = teDirTIF\n",
    "trWork, teWork = '../Work/Train', '../Work/Test'\n",
    "trDirI,teDirI, teDirJPG, trWork, teWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.read_csv(trLabels)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build list with unique labels\n",
    "label_list = []\n",
    "for tag_str in labels_df.tags.values:\n",
    "    labels = tag_str.split(' ')\n",
    "    for label in labels:\n",
    "        if label not in label_list:\n",
    "            label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add onehot features for every label\n",
    "for label in label_list:\n",
    "    labels_df[label] = labels_df['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n",
    "    #labels_df[label].astype(np.int8)\n",
    "# Display head\n",
    "#labels_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\n",
    "land_labels = ['primary', 'agriculture', 'water', 'cultivation', 'habitation' ]\n",
    "rare_labels = [l for l in label_list if labels_df[label_list].sum()[l] < 2000]\n",
    "#rare_labels              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = label_list; #weather_labels;\n",
    "nameList =labels_df[labels_df[labels].sum(axis=1)>0].image_name.tolist(); len(nameList)\n",
    "labelList=labels_df[labels_df[labels].sum(axis=1)>0][labels].as_matrix();\n",
    "labelList[:6,:]\n",
    "#labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#del(trOX); del(trOY);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##np.save('../Data-Keras/Datas/train-model-2D-128x128x3-XX-not-cloudy.npy',trX)\n",
    "##np.save('../Data-Keras/Datas/train-model-2D-128x128x3-YY-not-cloudy.npy',trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38390, 128, 128, 3) (38390, 16)\n"
     ]
    }
   ],
   "source": [
    "if False :\n",
    "    ###trXX = np.load('../Data-Keras/Datas/train-model-2D-64x64x6-XX.npy')\n",
    "    ###trYY = np.load('../Data-Keras/Datas/train-model-2D-64x64x6-YY.npy')\n",
    "    ######trXX  = np.load('../Data-Keras/Datas/train-model-2D-64x64x3-XX.npy')\n",
    "    ######trYY  = np.load('../Data-Keras/Datas/train-model-2D-64x64x3-YY.npy')\n",
    "    trXX  = np.load('../Data-Keras/Datas/train-model-2D-128x128x3-XX.npy')\n",
    "    trYY  = np.load('../Data-Keras/Datas/train-model-2D-128x128x3-YY.npy')\n",
    "    ######trXX  = np.load('../Data-Keras/Datas/train-model-2D-224x224x3-XX.npy')\n",
    "    ######trYY  = np.load('../Data-Keras/Datas/train-model-2D-224x224x3-YY.npy')\n",
    "\n",
    "    trX, trY = trXX[trYY[:,9]==0], trYY[trYY[:,9]==0] # not cloudy == 9 feature\n",
    "    trY=trY[:,range(0,9)+range(10,17)] # --cloudy <> 9\n",
    "    del trXX,trYY\n",
    "    #print(trXX.shape,trYY.shape)\n",
    "    print(trX.shape,trY.shape)\n",
    "if False :\n",
    "    trX  = np.load('../Data-Keras/Datas/train-model-2D-224x224x3-XX-short.npy')\n",
    "    trY  = np.load('../Data-Keras/Datas/train-model-2D-224x224x3-YY-short.npy')\n",
    "    print(trX.shape,trY.shape)\n",
    "if True :\n",
    "    trX  = np.load('../Data-Keras/Datas/train-model-2D-128x128x3-XX-not-cloudy.npy')\n",
    "    trY  = np.load('../Data-Keras/Datas/train-model-2D-128x128x3-YY-not-cloudy.npy')\n",
    "    print(trX.shape,trY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38390, 128, 128, 3) (38390, 16)\n"
     ]
    }
   ],
   "source": [
    "print(trX.shape,trY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#trX=trX/255.0\n",
    "##trX = cv.normalize(trX, trX, alpha=0, beta=1, norm_type=cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
    "#trX=trX/65535.0\n",
    "#trX[0,:,:]\n",
    "#trX[:,:,:,5].max(),trX.min(),trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fbeta_pred(y_true, y_pred, beta=2.0, OK1=0.2, eps=0.000001, printOK=False):\n",
    "    beta2 = beta*beta\n",
    "    yy_true = K.round(y_true)\n",
    "    #yy_pred = K.round(y_pred+(0.5-OK1))\n",
    "    yy_pred = K.round(y_pred)\n",
    "    tp, tp_fp, fn = K.sum((yy_pred*yy_true)), K.sum(yy_true), K.sum((K.abs(yy_pred*(yy_true-1.0))))\n",
    "    precision, recall = tp/(tp_fp+eps), tp/(tp+fn+eps) \n",
    "    fbeta = (1+beta2)*(precision*recall)/(beta2*precision+recall+eps)\n",
    "    ##if fbeta>1.0 : fbeta = 1.0;\n",
    "    if printOK :\n",
    "        print('ten true ',K.get_value(yy_true))\n",
    "        #print('ten pred ',y_pred)\n",
    "        print('ten roun ',K.get_value(yy_pred))\n",
    "        print(' pre=',K.get_value(precision),' recall=',K.get_value(recall),' tp=',\n",
    "              K.get_value(tp),' fn=',K.get_value(fn),' tp+fp=',K.get_value(tp_fp))\n",
    "    return(fbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# model Krizevsky 2012\n",
    "#\n",
    "#    input(224x224)\n",
    "#    c2d(96,(11,11),strides=(4,4))\n",
    "#    c2d(256,(5,5))\n",
    "#    c2d(384,(3,3))\n",
    "#    c2d(384,(3,3))\n",
    "#    c2d(256,(3,3))\n",
    "#    maxp(2,2)\n",
    "#    flaten()\n",
    "#    dense(4096)\n",
    "#    dense(4096)\n",
    "#    dense(output(1000))\n",
    "#\n",
    "#\n",
    "def Kriz2012 ( model, iSize, rSize ) :\n",
    "    i1,i2,i3 = iSize\n",
    "    model.add(BatchNormalization(input_shape=(i1,i2,i3)))\n",
    "    \n",
    "    model.add(Convolution2D(96,(11,11),strides=(4,4),activation='relu'))\n",
    "    model.add(Convolution2D(256,(5,5),activation='relu'))\n",
    "    model.add(Convolution2D(384,(3,3),activation='relu'))\n",
    "    model.add(Convolution2D(384,(3,3),activation='relu'))\n",
    "    model.add(Convolution2D(256,(3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(rSize,activation='sigmoid'))\n",
    "\n",
    "    return(model)\n",
    "\n",
    "#\n",
    "# model Krizevsky 2012\n",
    "#\n",
    "#    input(224x224)\n",
    "#    c2d(96,(11,11),strides=(4,4))\n",
    "#    c2d(256,(5,5))\n",
    "#    c2d(384,(3,3))\n",
    "#    c2d(384,(3,3))\n",
    "#    c2d(256,(3,3))\n",
    "#    maxp(2,2)\n",
    "#    flaten()\n",
    "#    dense(4096)\n",
    "#    dense(4096)\n",
    "#    dense(output(1000))\n",
    "#\n",
    "#\n",
    "def Kriz2012X ( model, iSize, rSize ) :\n",
    "    i1,i2,i3 = iSize\n",
    "    model.add(BatchNormalization(input_shape=(i1,i2,i3)))\n",
    "    \n",
    "    model.add(Convolution2D(96,(11,11),strides=(4,4),activation='relu'))\n",
    "    model.add(Convolution2D(256,(5,5),activation='relu'))\n",
    "    model.add(Convolution2D(384,(3,3),activation='relu'))\n",
    "    model.add(Convolution2D(384,(3,3),activation='relu'))\n",
    "    #model.add(Convolution2D(256,(3,3),activation='relu'))\n",
    "    model.add(Convolution2D(128,(3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(4096,activation='relu'))\n",
    "    model.add(Dense(1024,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    #model.add(Dense(4096,activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(rSize,activation='sigmoid'))\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# model Krizevsky 2012\n",
    "#\n",
    "#    input(224x224)\n",
    "#    c2d(96,(11,11),strides=(4,4))\n",
    "#    c2d(256,(5,5))\n",
    "#    c2d(384,(3,3))\n",
    "#    c2d(384,(3,3))\n",
    "#    c2d(256,(3,3))\n",
    "#    maxp(2,2)\n",
    "#    flaten()\n",
    "#    dense(4096)\n",
    "#    dense(4096)\n",
    "#    dense(output(1000))\n",
    "#\n",
    "# cut version\n",
    "#\n",
    "#\n",
    "def Kriz2012x3x3 ( model, iSize, rSize, cv2d, dense, pp ) :\n",
    "    i1,i2,i3 = iSize\n",
    "    model.add(BatchNormalization(axis=3,input_shape=(i1,i2,i3)))\n",
    "    \n",
    "    if not (cv2d is None) :\n",
    "        for cv in cv2d :\n",
    "            if (cv>0) :\n",
    "                model.add(Convolution2D(cv,(2,2)))\n",
    "                #model.add(BatchNormalization(axis=3))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "                model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if not (dense is None) :\n",
    "        for de in dense :\n",
    "            if (de>0) :\n",
    "                model.add(Dense(pp[5],activation='relu'))\n",
    "                model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(rSize,activation='sigmoid'))\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildModel (iSize,rSize,params=None,cv2d=None,dense=None) :\n",
    "    model = Sequential()\n",
    "    if (cv2d is None) and (dense is None) and not (params is None) : cv2d, dense = params[:-2], params[-2:]\n",
    "    model = Kriz2012x3x3(model,iSize,rSize,cv2d=cv2d,dense=dense,pp=params)\n",
    "    sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.99, nesterov=True)\n",
    "    model.compile(loss='mean_absolute_error',  #'binary_crossentropy',\n",
    "                  optimizer=\"adam\", #sgd, #\"adam\", #'rmsprop',\n",
    "                  metrics=[fbeta_pred,'acc']) #['binary_accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    #sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(loss='categorical_crossentropy',\n",
    "    #              optimizer='adam', #'adam', #sgd, #\"adam\", #'rmsprop',\n",
    "    #              metrics=['accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    return(model)\n",
    "\n",
    "def buildModelKriz (iSize,rSize,params=None,cv2d=None,dense=None) :\n",
    "    model = Sequential()\n",
    "    \n",
    "    model = Kriz2012(model,iSize,rSize)\n",
    "    #sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.99, nesterov=True)\n",
    "    sgd = keras.optimizers.SGD(nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', # 'mean_absolute_error'\n",
    "                  optimizer=\"sgd\", #sgd, #\"adam\", #'rmsprop',\n",
    "                  metrics=[fbeta_pred,'acc']) #['binary_accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    #sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(loss='categorical_crossentropy',\n",
    "    #              optimizer='adam', #'adam', #sgd, #\"adam\", #'rmsprop',\n",
    "    #              metrics=['accuracy']) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#mm1=np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "#mm2=np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "#mm1,mm2,mm1+mm2\n",
    "#len(result),len(resAll)\n",
    "#trX.shape,trX.dtype,trX[0]\n",
    "#trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "resAll = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3) 16\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=12,min_delta=0.0001)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)\n",
    "\n",
    "params = [(32,64,32,0,0,256,0),(32,64,16,0,0,256,0),(32,64,32,0,0,256,0),(32,64,64,0,0,256,0),(32,64,128,0,0,256,0),(32,64,256,0,0,256,0),(32,64,512,0,0,256,0),(32,64,1024,0,0,256,0)]\n",
    "params = params+[(32,64,8,0,0,16,0),(32,64,16,0,0,256,0),(32,64,32,0,0,64,0),(32,64,64,0,0,128,0),(32,64,128,0,0,256,0),(32,64,256,0,0,512,0),(32,64,512,0,0,1024,0),(32,64,1024,0,0,2048,0)]\n",
    "params = params+[(32,64,8,0,0,16,128),(32,64,16,0,0,32,128),(32,64,32,0,0,64,128),(32,64,64,0,0,128,128),(32,64,128,0,0,256,128),(32,64,256,0,0,512,128),(32,64,512,0,0,1024,128),(32,64,1024,0,0,2048,128)]\n",
    "params = params+[(96,256,384,384,256,4096,4096),(96,256,384,384,256,4096,2048),(96,256,384,384,256,2048,4096),(96,256,384,384,256,2048,2048),\n",
    "          (96,256,384,384,0,4096,0),(96,256,384,0,256,4096,0),(96,256,0,0,256,4096,0),\n",
    "          (96,256,0,0,0,4096,0),(96,0,0,0,0,4096,0),(96,256,0,0,256,4096,0)]\n",
    "#         (96,256,384,197,256,4096,0),  bad unchausted memory ResourceExhaustedError: OOM when allocating tensor with shape[128,8,8,384]\n",
    "#         (96,128,0,0,0,4096,0),\n",
    "#\n",
    "#        [(96,256,197,0,0,4096,0), (96,256,384,384,256,4096,0)]\n",
    "#\n",
    "#       [(96,32,0,0,0,1024,0),(96,64,0,0,0,1024,0),(96,128,0,0,0,1024,0),(96,256,0,0,0,1024,0),\n",
    "#         (256,32,0,0,0,1024,0),(256,64,0,0,0,1024,0),(256,128,0,0,0,1024,0)]\n",
    "\n",
    "params = [(96,64,0,0,0,256,0),(96,64,0,0,0,512,0),(96,64,0,0,0,1024,0),(96,64,0,0,0,2048,0),(96,64,0,0,0,8192,0)]\n",
    "params = params+[(16,64,0,0,0,256,0),(32,64,0,0,0,256,0),(64,64,0,0,0,256,0),(128,64,0,0,0,256,0),(256,64,0,0,0,256,0)]\n",
    "\n",
    "params = [(32,64,16,0,0,256,0)] \n",
    "params = [(16,32,64,256,0,512,512),(8,64,256,1024,0,512,512),(8,64,256,1024,0,0,512),(8,64,256,2048,0,0,512),(8,64,256,2048,0,512,512),\n",
    "          (8,64,256,2048,0,1024,0),(8,64,256,2048,0,1024,1024)]\n",
    "##params = [(8,64,256,1024,0,512,512),(8,64,256,1024,0,512,0)]\n",
    "params = [(8,64,256,2048,0,2048,0)] # 0.87 0.87\n",
    "params = [(8,64,256,4096,0,1024,0)] # 0.8686 0.8724\n",
    "params = [(32,64,16,0,0,256,512),(8,64,256,2048,0,1024,0),(8,64,256,2048,0,512,0),(8,64,256,4096,0,256,0)] # 0.875 0.8796\n",
    "params = [(32,64,16,0,0,256,512)]\n",
    "\n",
    "#params = [(8,64,256,4096,0,256,0)]\n",
    "\n",
    "iSize, rSize, result = (trX.shape[1],trX.shape[2],trX.shape[3]), trY.shape[1], []\n",
    "print(iSize,rSize)\n",
    "epochs  = 100\n",
    "verbose = 2\n",
    "batch_size = 64 # 64 0.8633 0.8717 (68 secs/batch) # 32 0.8545 0.8653 (87 secs/batch) # 16 0.8483 0.8623 (137 secs/batch) # 32\n",
    "for pp in [] : #params :\n",
    "    filepath=\"../Data-Keras/weights.{epoch:02d}-{val_loss:.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    model1 = buildModel(iSize,rSize,pp)\n",
    "    tr1=np.sum([K.count_params(p) for p in set(model1.trainable_weights)])\n",
    "    tr2=np.sum([K.count_params(p) for p in set(model1.non_trainable_weights)])\n",
    "    if (tr1+tr2)>14000000 : \n",
    "        print(datetime.datetime.now(),pp,tr1+tr2,'(badly)')\n",
    "        continue;\n",
    "    print(datetime.datetime.now(),pp,tr1+tr2)\n",
    "    hist1  = model1.fit(trX,trY,epochs=epochs, batch_size=batch_size, validation_split=0.20, \n",
    "                        callbacks=[early_stopping,reduce_lr,checkpoint],\n",
    "                        verbose=verbose)\n",
    "    \n",
    "    trP = model1.predict(trX, batch_size=128)\n",
    "    fbeta2score=fbeta_score(trY, np.array(trP) > 0.2, beta=2, average='samples')\n",
    "    fbeta2pred =K.get_value(fbeta_pred(trY.astype(np.float64),trP.astype(np.float64)))\n",
    "    #print(datetime.datetime.now(),pp,hist1.history['fbeta_pred'][-1],hist1.history['val_fbeta_pred'][-1],'fbeta2s=',fbeta2score,fbeta2pred)\n",
    "    print(datetime.datetime.now(),pp,'fbeta2s=',fbeta2score,fbeta2pred)\n",
    "    result.append([pp,hist1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 3) 16\n"
     ]
    }
   ],
   "source": [
    "iSize, rSize, result = (trX.shape[1],trX.shape[2],trX.shape[3]), trY.shape[1], []\n",
    "params = (32,64,128,256,0,256,512)\n",
    "print(iSize,rSize)\n",
    "metric = 'acc'\n",
    "model10 = buildModelKriz(iSize,rSize,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if False :\n",
    "    sgd = keras.optimizers.SGD(nesterov=True)\n",
    "    model10.compile(loss= 'binary_crossentropy', \n",
    "                #'mean_squared_error', #'mean_absolute_error',  #'binary_crossentropy','categorical_crossentropy',\n",
    "                  optimizer=\"rmsprop\", #sgd, #\"adam\", #'rmsprop',\n",
    "                  metrics=[metric,fbeta_pred]) #[fbeta_pred]) #['accuracy',fbeta_pred]) #['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model10.load_weights('../Data-Keras/Models/model-Alex-weights-128x128x3.h5', by_name=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-05 10:47:29.122955 125455772\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: acc improved from -inf to 0.87791, saving model to ../Temp/weights.00-acc=0.9061.hdf5\n",
      "251s - loss: 0.3849 - fbeta_pred: 0.7210 - acc: 0.8779 - val_loss: 0.2618 - val_fbeta_pred: 0.7889 - val_acc: 0.9061\n",
      "Epoch 2/100\n",
      "Epoch 00001: acc improved from 0.87791 to 0.90348, saving model to ../Temp/weights.01-acc=0.9061.hdf5\n",
      "309s - loss: 0.2509 - fbeta_pred: 0.7817 - acc: 0.9035 - val_loss: 0.2400 - val_fbeta_pred: 0.7889 - val_acc: 0.9061\n",
      "Epoch 3/100\n",
      "Epoch 00002: acc did not improve\n",
      "67s - loss: 0.2463 - fbeta_pred: 0.7818 - acc: 0.9035 - val_loss: 0.2390 - val_fbeta_pred: 0.7889 - val_acc: 0.9061\n",
      "Epoch 4/100\n",
      "Epoch 00003: acc improved from 0.90348 to 0.90351, saving model to ../Temp/weights.03-acc=0.9061.hdf5\n",
      "404s - loss: 0.2449 - fbeta_pred: 0.7820 - acc: 0.9035 - val_loss: 0.2374 - val_fbeta_pred: 0.7889 - val_acc: 0.9061\n",
      "Epoch 5/100\n",
      "Epoch 00004: acc did not improve\n",
      "85s - loss: 0.2435 - fbeta_pred: 0.7819 - acc: 0.9035 - val_loss: 0.2368 - val_fbeta_pred: 0.7889 - val_acc: 0.9061\n",
      "Epoch 6/100\n",
      "Epoch 00005: acc did not improve\n",
      "63s - loss: 0.2429 - fbeta_pred: 0.7819 - acc: 0.9035 - val_loss: 0.2360 - val_fbeta_pred: 0.7889 - val_acc: 0.9061\n",
      "Epoch 7/100\n",
      "Epoch 00006: acc did not improve\n",
      "63s - loss: 0.2416 - fbeta_pred: 0.7817 - acc: 0.9035 - val_loss: 0.2341 - val_fbeta_pred: 0.7889 - val_acc: 0.9061\n",
      "Epoch 8/100\n",
      "Epoch 00007: acc improved from 0.90351 to 0.90370, saving model to ../Temp/weights.07-acc=0.9060.hdf5\n",
      "175s - loss: 0.2390 - fbeta_pred: 0.7818 - acc: 0.9037 - val_loss: 0.2313 - val_fbeta_pred: 0.7883 - val_acc: 0.9060\n",
      "Epoch 9/100\n",
      "Epoch 00008: acc improved from 0.90370 to 0.90425, saving model to ../Temp/weights.08-acc=0.9065.hdf5\n",
      "79s - loss: 0.2350 - fbeta_pred: 0.7809 - acc: 0.9042 - val_loss: 0.2270 - val_fbeta_pred: 0.7837 - val_acc: 0.9065\n",
      "Epoch 10/100\n",
      "Epoch 00009: acc improved from 0.90425 to 0.90612, saving model to ../Temp/weights.09-acc=0.9117.hdf5\n",
      "75s - loss: 0.2308 - fbeta_pred: 0.7806 - acc: 0.9061 - val_loss: 0.2231 - val_fbeta_pred: 0.7948 - val_acc: 0.9117\n",
      "Epoch 11/100\n",
      "Epoch 00010: acc improved from 0.90612 to 0.90821, saving model to ../Temp/weights.10-acc=0.9121.hdf5\n",
      "74s - loss: 0.2269 - fbeta_pred: 0.7847 - acc: 0.9082 - val_loss: 0.2212 - val_fbeta_pred: 0.7919 - val_acc: 0.9121\n",
      "Epoch 12/100\n",
      "Epoch 00011: acc improved from 0.90821 to 0.90873, saving model to ../Temp/weights.11-acc=0.9121.hdf5\n",
      "74s - loss: 0.2249 - fbeta_pred: 0.7846 - acc: 0.9087 - val_loss: 0.2201 - val_fbeta_pred: 0.7891 - val_acc: 0.9121\n",
      "Epoch 13/100\n",
      "Epoch 00012: acc improved from 0.90873 to 0.90886, saving model to ../Temp/weights.12-acc=0.9120.hdf5\n",
      "67s - loss: 0.2241 - fbeta_pred: 0.7841 - acc: 0.9089 - val_loss: 0.2194 - val_fbeta_pred: 0.7874 - val_acc: 0.9120\n",
      "Epoch 14/100\n",
      "Epoch 00013: acc improved from 0.90886 to 0.90910, saving model to ../Temp/weights.13-acc=0.9130.hdf5\n",
      "74s - loss: 0.2231 - fbeta_pred: 0.7843 - acc: 0.9091 - val_loss: 0.2189 - val_fbeta_pred: 0.7934 - val_acc: 0.9130\n",
      "Epoch 15/100\n",
      "Epoch 00014: acc did not improve\n",
      "62s - loss: 0.2222 - fbeta_pred: 0.7834 - acc: 0.9088 - val_loss: 0.2184 - val_fbeta_pred: 0.7868 - val_acc: 0.9119\n",
      "Epoch 16/100\n",
      "Epoch 00015: acc did not improve\n",
      "62s - loss: 0.2218 - fbeta_pred: 0.7834 - acc: 0.9091 - val_loss: 0.2183 - val_fbeta_pred: 0.7882 - val_acc: 0.9119\n",
      "Epoch 17/100\n",
      "Epoch 00016: acc improved from 0.90910 to 0.90940, saving model to ../Temp/weights.16-acc=0.9123.hdf5\n",
      "67s - loss: 0.2211 - fbeta_pred: 0.7845 - acc: 0.9094 - val_loss: 0.2178 - val_fbeta_pred: 0.7896 - val_acc: 0.9123\n",
      "Epoch 18/100\n",
      "Epoch 00017: acc did not improve\n",
      "61s - loss: 0.2207 - fbeta_pred: 0.7835 - acc: 0.9092 - val_loss: 0.2176 - val_fbeta_pred: 0.7887 - val_acc: 0.9123\n",
      "Epoch 19/100\n",
      "Epoch 00018: acc improved from 0.90940 to 0.90984, saving model to ../Temp/weights.18-acc=0.9125.hdf5\n",
      "65s - loss: 0.2205 - fbeta_pred: 0.7854 - acc: 0.9098 - val_loss: 0.2171 - val_fbeta_pred: 0.7895 - val_acc: 0.9125\n",
      "Epoch 20/100\n",
      "Epoch 00019: acc did not improve\n",
      "61s - loss: 0.2203 - fbeta_pred: 0.7832 - acc: 0.9092 - val_loss: 0.2167 - val_fbeta_pred: 0.7944 - val_acc: 0.9134\n",
      "Epoch 21/100\n",
      "Epoch 00020: acc did not improve\n",
      "61s - loss: 0.2197 - fbeta_pred: 0.7834 - acc: 0.9092 - val_loss: 0.2165 - val_fbeta_pred: 0.7909 - val_acc: 0.9129\n",
      "Epoch 22/100\n",
      "Epoch 00021: acc did not improve\n",
      "61s - loss: 0.2193 - fbeta_pred: 0.7847 - acc: 0.9097 - val_loss: 0.2165 - val_fbeta_pred: 0.7920 - val_acc: 0.9130\n",
      "Epoch 23/100\n",
      "Epoch 00022: acc did not improve\n",
      "61s - loss: 0.2190 - fbeta_pred: 0.7848 - acc: 0.9096 - val_loss: 0.2164 - val_fbeta_pred: 0.7920 - val_acc: 0.9128\n",
      "Epoch 24/100\n",
      "Epoch 00023: acc did not improve\n",
      "61s - loss: 0.2187 - fbeta_pred: 0.7855 - acc: 0.9098 - val_loss: 0.2158 - val_fbeta_pred: 0.7889 - val_acc: 0.9126\n",
      "Epoch 25/100\n",
      "Epoch 00024: acc improved from 0.90984 to 0.91012, saving model to ../Temp/weights.24-acc=0.9140.hdf5\n",
      "65s - loss: 0.2184 - fbeta_pred: 0.7856 - acc: 0.9101 - val_loss: 0.2156 - val_fbeta_pred: 0.7985 - val_acc: 0.9140\n",
      "Epoch 26/100\n",
      "Epoch 00025: acc did not improve\n",
      "61s - loss: 0.2180 - fbeta_pred: 0.7865 - acc: 0.9101 - val_loss: 0.2153 - val_fbeta_pred: 0.7897 - val_acc: 0.9127\n",
      "Epoch 27/100\n",
      "Epoch 00026: acc improved from 0.91012 to 0.91023, saving model to ../Temp/weights.26-acc=0.9133.hdf5\n",
      "65s - loss: 0.2176 - fbeta_pred: 0.7867 - acc: 0.9102 - val_loss: 0.2151 - val_fbeta_pred: 0.7926 - val_acc: 0.9133\n",
      "Epoch 28/100\n",
      "Epoch 00027: acc did not improve\n",
      "61s - loss: 0.2173 - fbeta_pred: 0.7864 - acc: 0.9101 - val_loss: 0.2150 - val_fbeta_pred: 0.7879 - val_acc: 0.9124\n",
      "Epoch 29/100\n",
      "Epoch 00028: acc improved from 0.91023 to 0.91109, saving model to ../Temp/weights.28-acc=0.9136.hdf5\n",
      "66s - loss: 0.2165 - fbeta_pred: 0.7894 - acc: 0.9111 - val_loss: 0.2145 - val_fbeta_pred: 0.7936 - val_acc: 0.9136\n",
      "Epoch 30/100\n",
      "Epoch 00029: acc did not improve\n",
      "61s - loss: 0.2167 - fbeta_pred: 0.7881 - acc: 0.9107 - val_loss: 0.2144 - val_fbeta_pred: 0.7921 - val_acc: 0.9133\n",
      "Epoch 31/100\n",
      "Epoch 00030: acc did not improve\n",
      "61s - loss: 0.2163 - fbeta_pred: 0.7871 - acc: 0.9103 - val_loss: 0.2142 - val_fbeta_pred: 0.7930 - val_acc: 0.9134\n",
      "Epoch 32/100\n",
      "Epoch 00031: acc did not improve\n",
      "61s - loss: 0.2159 - fbeta_pred: 0.7877 - acc: 0.9106 - val_loss: 0.2135 - val_fbeta_pred: 0.7972 - val_acc: 0.9143\n",
      "Epoch 33/100\n",
      "Epoch 00032: acc improved from 0.91109 to 0.91116, saving model to ../Temp/weights.32-acc=0.9141.hdf5\n",
      "67s - loss: 0.2158 - fbeta_pred: 0.7898 - acc: 0.9112 - val_loss: 0.2132 - val_fbeta_pred: 0.7961 - val_acc: 0.9141\n",
      "Epoch 34/100\n",
      "Epoch 00033: acc did not improve\n",
      "61s - loss: 0.2150 - fbeta_pred: 0.7894 - acc: 0.9111 - val_loss: 0.2132 - val_fbeta_pred: 0.7954 - val_acc: 0.9139\n",
      "Epoch 35/100\n",
      "Epoch 00034: acc improved from 0.91116 to 0.91139, saving model to ../Temp/weights.34-acc=0.9134.hdf5\n",
      "66s - loss: 0.2145 - fbeta_pred: 0.7901 - acc: 0.9114 - val_loss: 0.2130 - val_fbeta_pred: 0.7928 - val_acc: 0.9134\n",
      "Epoch 36/100\n",
      "Epoch 00035: acc did not improve\n",
      "61s - loss: 0.2141 - fbeta_pred: 0.7910 - acc: 0.9114 - val_loss: 0.2125 - val_fbeta_pred: 0.7940 - val_acc: 0.9139\n",
      "Epoch 37/100\n",
      "Epoch 00036: acc improved from 0.91139 to 0.91144, saving model to ../Temp/weights.36-acc=0.9148.hdf5\n",
      "66s - loss: 0.2138 - fbeta_pred: 0.7901 - acc: 0.9114 - val_loss: 0.2120 - val_fbeta_pred: 0.7989 - val_acc: 0.9148\n",
      "Epoch 38/100\n",
      "Epoch 00037: acc improved from 0.91144 to 0.91182, saving model to ../Temp/weights.37-acc=0.9145.hdf5\n",
      "66s - loss: 0.2139 - fbeta_pred: 0.7924 - acc: 0.9118 - val_loss: 0.2116 - val_fbeta_pred: 0.7975 - val_acc: 0.9145\n",
      "Epoch 39/100\n",
      "Epoch 00038: acc improved from 0.91182 to 0.91210, saving model to ../Temp/weights.38-acc=0.9151.hdf5\n",
      "67s - loss: 0.2133 - fbeta_pred: 0.7930 - acc: 0.9121 - val_loss: 0.2113 - val_fbeta_pred: 0.8034 - val_acc: 0.9151\n",
      "Epoch 40/100\n",
      "Epoch 00039: acc improved from 0.91210 to 0.91251, saving model to ../Temp/weights.39-acc=0.9143.hdf5\n",
      "66s - loss: 0.2128 - fbeta_pred: 0.7943 - acc: 0.9125 - val_loss: 0.2110 - val_fbeta_pred: 0.7953 - val_acc: 0.9143\n",
      "Epoch 41/100\n",
      "Epoch 00040: acc improved from 0.91251 to 0.91259, saving model to ../Temp/weights.40-acc=0.9149.hdf5\n",
      "66s - loss: 0.2120 - fbeta_pred: 0.7938 - acc: 0.9126 - val_loss: 0.2113 - val_fbeta_pred: 0.8040 - val_acc: 0.9149\n",
      "Epoch 42/100\n",
      "Epoch 00041: acc improved from 0.91259 to 0.91309, saving model to ../Temp/weights.41-acc=0.9159.hdf5\n",
      "66s - loss: 0.2114 - fbeta_pred: 0.7958 - acc: 0.9131 - val_loss: 0.2100 - val_fbeta_pred: 0.8025 - val_acc: 0.9159\n",
      "Epoch 43/100\n",
      "Epoch 00042: acc did not improve\n",
      "61s - loss: 0.2110 - fbeta_pred: 0.7943 - acc: 0.9127 - val_loss: 0.2099 - val_fbeta_pred: 0.8060 - val_acc: 0.9166\n",
      "Epoch 44/100\n",
      "Epoch 00043: acc did not improve\n",
      "61s - loss: 0.2110 - fbeta_pred: 0.7942 - acc: 0.9128 - val_loss: 0.2097 - val_fbeta_pred: 0.7964 - val_acc: 0.9151\n",
      "Epoch 45/100\n",
      "Epoch 00044: acc improved from 0.91309 to 0.91327, saving model to ../Temp/weights.44-acc=0.9153.hdf5\n",
      "65s - loss: 0.2099 - fbeta_pred: 0.7959 - acc: 0.9133 - val_loss: 0.2092 - val_fbeta_pred: 0.7960 - val_acc: 0.9153\n",
      "Epoch 46/100\n",
      "Epoch 00045: acc improved from 0.91327 to 0.91330, saving model to ../Temp/weights.45-acc=0.9168.hdf5\n",
      "66s - loss: 0.2094 - fbeta_pred: 0.7960 - acc: 0.9133 - val_loss: 0.2087 - val_fbeta_pred: 0.7980 - val_acc: 0.9168\n",
      "Epoch 47/100\n",
      "Epoch 00046: acc improved from 0.91330 to 0.91368, saving model to ../Temp/weights.46-acc=0.9163.hdf5\n",
      "65s - loss: 0.2093 - fbeta_pred: 0.7970 - acc: 0.9137 - val_loss: 0.2084 - val_fbeta_pred: 0.8046 - val_acc: 0.9163\n",
      "Epoch 48/100\n",
      "Epoch 00047: acc improved from 0.91368 to 0.91418, saving model to ../Temp/weights.47-acc=0.9163.hdf5\n",
      "66s - loss: 0.2085 - fbeta_pred: 0.7980 - acc: 0.9142 - val_loss: 0.2085 - val_fbeta_pred: 0.7968 - val_acc: 0.9163\n",
      "Epoch 49/100\n",
      "Epoch 00048: acc improved from 0.91418 to 0.91445, saving model to ../Temp/weights.48-acc=0.9165.hdf5\n",
      "66s - loss: 0.2078 - fbeta_pred: 0.7988 - acc: 0.9145 - val_loss: 0.2074 - val_fbeta_pred: 0.7982 - val_acc: 0.9165\n",
      "Epoch 50/100\n",
      "Epoch 00049: acc did not improve\n",
      "61s - loss: 0.2072 - fbeta_pred: 0.7984 - acc: 0.9144 - val_loss: 0.2090 - val_fbeta_pred: 0.7974 - val_acc: 0.9157\n",
      "Epoch 51/100\n",
      "Epoch 00050: acc improved from 0.91445 to 0.91495, saving model to ../Temp/weights.50-acc=0.9155.hdf5\n",
      "65s - loss: 0.2065 - fbeta_pred: 0.8001 - acc: 0.9149 - val_loss: 0.2081 - val_fbeta_pred: 0.7949 - val_acc: 0.9155\n",
      "Epoch 52/100\n",
      "Epoch 00051: acc improved from 0.91495 to 0.91561, saving model to ../Temp/weights.51-acc=0.9181.hdf5\n",
      "66s - loss: 0.2058 - fbeta_pred: 0.8019 - acc: 0.9156 - val_loss: 0.2057 - val_fbeta_pred: 0.8063 - val_acc: 0.9181\n",
      "Epoch 53/100\n",
      "Epoch 00052: acc did not improve\n",
      "61s - loss: 0.2056 - fbeta_pred: 0.8010 - acc: 0.9152 - val_loss: 0.2054 - val_fbeta_pred: 0.8073 - val_acc: 0.9178\n",
      "Epoch 54/100\n",
      "Epoch 00053: acc improved from 0.91561 to 0.91605, saving model to ../Temp/weights.53-acc=0.9180.hdf5\n",
      "66s - loss: 0.2049 - fbeta_pred: 0.8033 - acc: 0.9160 - val_loss: 0.2054 - val_fbeta_pred: 0.8127 - val_acc: 0.9180\n",
      "Epoch 55/100\n",
      "Epoch 00054: acc did not improve\n",
      "61s - loss: 0.2042 - fbeta_pred: 0.8012 - acc: 0.9155 - val_loss: 0.2046 - val_fbeta_pred: 0.8058 - val_acc: 0.9185\n",
      "Epoch 56/100\n",
      "Epoch 00055: acc did not improve\n",
      "61s - loss: 0.2039 - fbeta_pred: 0.8024 - acc: 0.9159 - val_loss: 0.2066 - val_fbeta_pred: 0.7951 - val_acc: 0.9167\n",
      "Epoch 57/100\n",
      "Epoch 00056: acc did not improve\n",
      "61s - loss: 0.2036 - fbeta_pred: 0.8013 - acc: 0.9155 - val_loss: 0.2040 - val_fbeta_pred: 0.8074 - val_acc: 0.9188\n",
      "Epoch 58/100\n",
      "Epoch 00057: acc improved from 0.91605 to 0.91644, saving model to ../Temp/weights.57-acc=0.9187.hdf5\n",
      "66s - loss: 0.2030 - fbeta_pred: 0.8039 - acc: 0.9164 - val_loss: 0.2037 - val_fbeta_pred: 0.8105 - val_acc: 0.9187\n",
      "Epoch 59/100\n",
      "Epoch 00058: acc improved from 0.91644 to 0.91730, saving model to ../Temp/weights.58-acc=0.9168.hdf5\n",
      "71s - loss: 0.2021 - fbeta_pred: 0.8064 - acc: 0.9173 - val_loss: 0.2057 - val_fbeta_pred: 0.8155 - val_acc: 0.9168\n",
      "Epoch 60/100\n",
      "Epoch 00059: acc did not improve\n",
      "61s - loss: 0.2017 - fbeta_pred: 0.8046 - acc: 0.9167 - val_loss: 0.2080 - val_fbeta_pred: 0.7865 - val_acc: 0.9153\n",
      "Epoch 61/100\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3,min_delta=0.0001)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_lr=0.001)\n",
    "\n",
    "epochs     = 100\n",
    "verbose    = 2\n",
    "batch_size = 64\n",
    "\n",
    "filepath=\"../Temp/weights.{epoch:02d}-acc={\"+metric+\":.4f}-val_acc={val_\"+metric+\":.4f}.hdf5\"\n",
    "##checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_'+metric, verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=metric, verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "##model1 = buildModel(iSize,rSize,pp)\n",
    "tr1    = np.sum([K.count_params(p) for p in set(model10.trainable_weights)])\n",
    "tr2    = np.sum([K.count_params(p) for p in set(model10.non_trainable_weights)])\n",
    "\n",
    "#print(tr1,tr2,tr1+tr2)\n",
    "#assert ((tr1+tr2)>14000000)\n",
    "\n",
    "step = 10000\n",
    "low  = 16000\n",
    "high = low+step\n",
    "\n",
    "print(datetime.datetime.now(),tr1+tr2)\n",
    "#hist1  = model10.fit(trX[low:high],trY[low:high],\n",
    "hist1  = model10.fit(trX[low:high],trY[low:high],\n",
    "                    epochs=epochs, batch_size=batch_size, \n",
    "                    validation_split=0.20, \n",
    "                    callbacks=[early_stopping,reduce_lr,checkpoint],\n",
    "                    verbose=verbose)\n",
    "\n",
    "##trP = model1.predict(trX, batch_size=128)\n",
    "##fbeta2score=fbeta_score(trY, np.array(trP) > 0.2, beta=2, average='samples')\n",
    "##fbeta2pred =K.get_value(fbeta_pred(trY.astype(np.float64),trP.astype(np.float64)))\n",
    "print(datetime.datetime.now()) #,pp,'fbeta2s=',fbeta2score,fbeta2pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trX.shape, trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.1 s, sys: 4.36 s, total: 27.5 s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%time trP = model10.predict(trX, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5453121793055851, 0.90393331599254167]\n",
      "(38390, 16)\n",
      "(38390, 16)\n",
      "fbeta_score= 0.677929321583\n",
      "fbeta_pred = 0.782743868033\n"
     ]
    }
   ],
   "source": [
    "print(model10.evaluate(trX,trY,verbose=2))\n",
    "print(trY.shape)\n",
    "print(trP.shape)\n",
    "print('fbeta_score=',fbeta_score(trY, np.array(trP) > 0.2, beta=2, average='samples'))\n",
    "print('fbeta_pred =',K.get_value(fbeta_pred(trY.astype(np.float64),trP.astype(np.float64))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "resAll = resAll+result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if True : #False :\n",
    "    save_model(model10,'../Data-Keras/Models/model-Alex-128x128x3.h5')\n",
    "    model10.save_weights('../Data-Keras/Models/model-Alex-weights-128x128x3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 128, 128, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 96)        34944     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 256)       614656    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 22, 22, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              104861696 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                65552     \n",
      "=================================================================\n",
      "Total params: 125,455,772\n",
      "Trainable params: 125,455,766\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-20 05:22:01.009723 0 --> 70 0.758419290079 [0.75741733368803943, 0.75841929007857778, 0.75785138394714635]\n",
      "2017-05-20 05:22:23.972821 1 --> 78 0.982638483662 [0.98262473453994426, 0.98263848366246676, 0.98259679666709321]\n",
      "2017-05-20 05:22:52.632376 2 --> 58 0.847226297632 [0.8469111862002846, 0.84722629763157453, 0.84682213028656683]\n",
      "2017-05-20 05:23:27.033547 3 --> 78 0.97604678248 [0.97585536488705449, 0.97604678247993626, 0.97586138532621902]\n",
      "2017-05-20 05:24:07.288213 4 --> 39 0.78729476795 [0.78664609854346335, 0.78729476795032916, 0.78707371492899492]\n",
      "2017-05-20 05:24:52.980975 5 --> 42 0.789568102568 [0.78871126051941221, 0.78956810256787635, 0.78669767415698355]\n",
      "2017-05-20 05:25:44.022237 6 --> 47 0.830831644043 [0.8305167161398419, 0.83083164404299403, 0.83015444819651052]\n",
      "2017-05-20 05:26:40.712293 7 --> 45 0.738016164169 [0.73686966073098015, 0.73801616416924898, 0.73773213859960107]\n",
      "2017-05-20 05:27:43.679293 8 --> 26 0.676190170231 [0.6690774577597115, 0.6761901702314258, 0.65439641541326066]\n",
      "2017-05-20 05:28:52.203737 9 --> 99 0.860687873635 [0.84540789671187133, 0.86068787363532673]\n",
      "2017-05-20 05:30:06.360727 10 --> 53 0.964034283738 [0.96339687009527564, 0.96403428373772715, 0.96364205060652686]\n",
      "2017-05-20 05:31:26.352144 11 --> 30 0.900537376612 [0.89285689393748568, 0.90053737661152566, 0.90053737661152566]\n",
      "2017-05-20 05:32:51.552201 12 --> 27 0.652844152686 [0.64595635259618922, 0.6528441526860862, 0.65053736636295179]\n",
      "2017-05-20 05:34:22.405562 13 --> 24 0.863599425241 [0.86179482608466784, 0.86359942524081812, 0.85725979475579062]\n",
      "2017-05-20 05:35:59.177670 14 --> 14 0.370370055203 [0.3600540358709392, 0.37037005520335736, 0.35595995253798635]\n",
      "2017-05-20 05:37:41.803909 15 --> 22 0.683823292523 [0.67335220282476826, 0.68382329252278806, 0.67567543551442677]\n",
      "2017-05-20 05:39:29.432537 16 --> 14 0.589171696773 [0.56748439106677917, 0.58917169677281178, 0.5851060891681884]\n"
     ]
    }
   ],
   "source": [
    "rr, rrx = [], []; trP = model.predict(trX, batch_size=128)\n",
    "for i in range(trP.shape[1]) :\n",
    "    xx = [];\n",
    "    trYY = trY[:,i].astype(np.float64)\n",
    "    trPY = trP[:,i]\n",
    "    for ii in range(100) :\n",
    "        trPP = (trPY>0.01*ii).astype(np.float64)\n",
    "        #x = fbeta_score(trY[:,0], np.array(trP[:,0] > 0.1*ii), beta=2, average='samples')\n",
    "        x = K.get_value(fbeta_pred(trYY,trPP))\n",
    "        xx.append(x)\n",
    "    rrr = np.array(xx).argmax();\n",
    "    rr.append(rrr)\n",
    "    rrx.append(xx[rrr])\n",
    "    print(datetime.datetime.now(),i,'-->',rrr,xx[rrr],xx[(rrr-1):(rrr+2)])\n",
    "    #print(xx);\n",
    "    #plt.plot(np.array(xx)); plt.show()\n",
    "trM = np.array(rr)/100.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('../Data-Keras/train-model-2D-2-v2-loop-weights.h5') ## verify load weights from v1 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Forming output dataset for predicting --> trOX, trOY\n",
    "del(trX)\n",
    "del(trY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61191\n",
      "2017-05-20 05:46:19.287685 61191 61191\n",
      "2017-05-20 05:46:38.218814 \t 0 \t 5000 \t test_14523.jpg \t \n",
      "2017-05-20 05:46:51.023731 \t 1 \t 10000 \t test_19029.jpg \t (10000, 17)\n",
      "2017-05-20 05:46:58.884016 \t 1 \t 15000 \t test_23524.jpg \t (10000, 17)\n",
      "2017-05-20 05:47:09.507695 \t 2 \t 20000 \t test_28015.jpg \t (20000, 17)\n",
      "2017-05-20 05:47:16.475888 \t 2 \t 25000 \t test_32520.jpg \t (20000, 17)\n",
      "2017-05-20 05:47:26.837760 \t 3 \t 30000 \t test_37026.jpg \t (30000, 17)\n",
      "2017-05-20 05:47:33.858034 \t 3 \t 35000 \t test_4908.jpg \t (30000, 17)\n",
      "2017-05-20 05:47:44.019579 \t 4 \t 40000 \t test_9402.jpg \t (40000, 17)\n",
      "2017-05-20 05:47:51.298769 \t 4 \t 45000 \t file_13913.jpg \t (40000, 17)\n",
      "2017-05-20 05:48:01.555641 \t 5 \t 50000 \t file_18419.jpg \t (50000, 17)\n",
      "2017-05-20 05:48:24.538867 \t 5 \t 55000 \t file_4564.jpg \t (50000, 17)\n",
      "2017-05-20 05:48:40.785101 \t 6 \t 60000 \t file_892.jpg \t (60000, 17)\n",
      "2017-05-20 05:48:43.863555\n"
     ]
    }
   ],
   "source": [
    "#nameAsk = os.listdir(teDirI); print(len(nameAsk))\n",
    "nameAsk = os.listdir(teDirJPG); print(len(nameAsk))\n",
    "trOX, trOY, i, ii, size = [], [], 0, 0, len(nameAsk)\n",
    "print(datetime.datetime.now(),len(nameAsk),size)\n",
    "for nn in nameAsk[0:size] :\n",
    "    #nf = os.path.join(teDirTIF,nn);\n",
    "    nf = os.path.join(teDirJPG,nn);\n",
    "    nx = formImExt(nf,resize=(64,64))\n",
    "    if (nx is not None) :\n",
    "        trOX.append(nx)\n",
    "        trOY.append(nn)\n",
    "    i += 1\n",
    "    if (i%10000==0) and (i>1) :\n",
    "        if (ii==0) :\n",
    "            trOX = np.array(trOX); trOX = trOX / 255.0\n",
    "            trP = model.predict(trOX, batch_size=512); \n",
    "        else :\n",
    "            trOX = np.array(trOX);  trOX = trOX / 255.0\n",
    "            trP = np.vstack([trP,model.predict(trOX, batch_size=512)]); \n",
    "        trOX,ii = [],ii+1;\n",
    "    if (i%5000==0) : print(datetime.datetime.now(),\"\\t\",ii,'\\t',i,\"\\t\",nn,'\\t',(trP.shape if ii>0 else \"\"))\n",
    "\n",
    "if (len(trOX)>0) :\n",
    "    if (ii==0) :\n",
    "        trOX = np.array(trOX); trOX = trOX / 255.0\n",
    "        trP = model.predict(trOX, batch_size=512); \n",
    "    else :\n",
    "        trOX = np.array(trOX); trOX = trOX / 255.0\n",
    "        trP = np.vstack([trP,model.predict(trOX, batch_size=512)]); \n",
    "    trOX,ii = [],ii+1;\n",
    "    \n",
    "print(datetime.datetime.now())\n",
    "\n",
    "#assert (size!=len(trOY)), \"Wrong files {} != {}\".format(size,len(trOY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61191 (61191, 17) (61191,)\n"
     ]
    }
   ],
   "source": [
    "#trOX = np.array(trOX);\n",
    "trOY = np.array([os.path.splitext(x)[0] for x in trOY]);\n",
    "print(len(nameAsk),trP.shape,trOY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.save('../Data-Keras/test-basin-2D-64x64-OX-tif-v2.npy',trOX)\n",
    "np.save('../Data-Keras/test-basin-2D-64x64-OY-tif-v2.npy',trOY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 32, 32, 6) (61191,)\n"
     ]
    }
   ],
   "source": [
    "trOX = np.load('../Data-Keras/test-basin-2D-64x64-OX-tif-v2.npy')\n",
    "trOY = np.load('../Data-Keras/test-basin-2D-64x64-OY-tif-v2.npy')\n",
    "print(trOX.shape,trOY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.75841929007857778,\n",
       " 1: 0.98263848366246676,\n",
       " 2: 0.84722629763157453,\n",
       " 3: 0.97604678247993626,\n",
       " 4: 0.78729476795032916,\n",
       " 5: 0.78956810256787635,\n",
       " 6: 0.83083164404299403,\n",
       " 7: 0.73801616416924898,\n",
       " 8: 0.6761901702314258,\n",
       " 9: 0.86068787363532673,\n",
       " 10: 0.96403428373772715,\n",
       " 11: 0.90053737661152566,\n",
       " 12: 0.6528441526860862,\n",
       " 13: 0.86359942524081812,\n",
       " 14: 0.37037005520335736,\n",
       " 15: 0.68382329252278806,\n",
       " 16: 0.58917169677281178}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrx\n",
    "rrd=dict()\n",
    "for i in range(len(rrx)) :\n",
    "    rrd[i]=rrx[i]\n",
    "rrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Forming result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61191, 17), array([  6.56801313e-02,   7.46541142e-01,   2.32726157e-01,\n",
       "          9.55386758e-01,   1.95543230e-01,   9.11955476e-01,\n",
       "          8.89197826e-01,   2.53069662e-02,   1.54045611e-05,\n",
       "          7.43342913e-04,   6.54759035e-02,   6.17289741e-04,\n",
       "          8.51887614e-02,   2.95602629e-04,   2.64362683e-07,\n",
       "          1.03749386e-04,   2.92145728e-07], dtype=float32))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trP = model.predict(trOX, batch_size=512); \n",
    "trP.shape, trP[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,)\n",
      "[3, 10, 0, 9] \n",
      " ['haze', 'primary', 'agriculture', 'clear', 'water', 'habitation', 'road', 'cultivation', 'slash_burn', 'cloudy', 'partly_cloudy', 'conventional_mine', 'bare_ground', 'artisinal_mine', 'blooming', 'selective_logging', 'blow_down'] \n",
      " [2.0, 0.5, 0.5, 2.0, 0.5, 0.5, 0.5, 0.5, 0.5, 2.0, 2.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "trM=np.array([0.0]*len(labels)); print(trM.shape)\n",
    "wr = [labels.index(i) for i in weather_labels];\n",
    "tt=trM; trM[:]=0.5\n",
    "trM[np.array(wr)] = 2.0\n",
    "print(wr,'\\n',labels,'\\n',trM.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_0', 'clear primary'] \n",
      " ['file_20521', 'clear primary'] \n",
      " [ 2.   0.5  0.5  2.   0.5  0.5  0.5  0.5  0.5  2.   2.   0.5  0.5  0.5  0.5\n",
      "  0.5  0.5] [  1.63650557e-01   3.91220033e-01   1.36160061e-01   9.47374701e-01\n",
      "   9.09343541e-01   1.55211976e-02   3.13203782e-02   1.42206885e-02\n",
      "   8.06170846e-08   9.10553038e-01   7.85041321e-03   4.12757437e-08\n",
      "   6.71853591e-03   1.33155788e-06   7.46736941e-06   9.65268737e-06\n",
      "   7.95199711e-08]\n"
     ]
    }
   ],
   "source": [
    "#trP = model.predict(trX, batch_size=512); trP=K.get_value(trP)\n",
    "res = []\n",
    "\n",
    "for i in range(trP.shape[0]) :\n",
    "    trPP = [weather_labels[trP[i,wr].argmax()]] + [labels[ii] for ii in range(len(labels)) if (trP[i,ii]>trM[ii])];\n",
    "    pp   = ' '.join(trPP)\n",
    "    ##if (pp==\"\") : print(trY[i])\n",
    "    res.append([trOY[i],pp])\n",
    "\n",
    "res.sort(cmp=lambda x,y: cmp(int(x[0].partition('_')[2]),int(y[0].partition('_')[2])) if (x[0].partition('_')[0]==y[0].partition('_')[0]) else cmp(y[0].partition('_')[0],x[0].partition('_')[0]))\n",
    "#print(res[4:8],'\\n',res[-4:])\n",
    "print(res[0],'\\n',res[-1],'\\n',trM,trP[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['haze', 'primary', 'agriculture', 'clear', 'water', 'habitation', 'road', 'cultivation', 'slash_burn', 'cloudy', 'partly_cloudy', 'conventional_mine', 'bare_ground', 'artisinal_mine', 'blooming', 'selective_logging', 'blow_down']\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "#print(trM.tolist())\n",
    "#np.round(trP[4:11,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-20-05-51-18\n"
     ]
    }
   ],
   "source": [
    "rrr=pd.DataFrame(res,columns=['image_name','tags']); rrr.head(); \n",
    "suffixDT = (datetime.datetime.now()).strftime('%Y-%m-%d-%H-%M-%S'); print(suffixDT)\n",
    "rrr.to_csv('../Result/vss'+suffixDT+'.csv',index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
